# 深度学习入门
## 第二章 感知机
### 2.1 感知机是什么
**感知机**接收多个输入信号，输出一个信号。这里所说的“信号”可以想 象成电流或河流那样具备“流动性”的东西。像电流流过导线，向前方输送 电子一样，感知机的信号也会形成流，向前方输送信息。但是，和实际的电 流不同的是，感知机的信号只有“流/不流”（1/0）两种取值。在本书中，0 对应“不传递信号”，1对应“传递信号”。
$${y_i} = {w_i}{x_i} + ... + {w_n}{x_n} + {b_i}$$
![[2C`5]_W1_@Q})QR[~L8Z7ZJ.png]]
![[%0PZ9YG`%24]J84}QT%RV8R.png]]
### 2.2 感知机的实现
使用使用权重和偏置实现**与门**。
``` python
def AND(x1, x2):  
    x = np.array([x1, x2])  
    w = np.array([0.5, 0.5])  
    b = -0.7  
    tmp = np.sum(w*x) + b  
    if tmp <= 0:  
        return 0  
    else:  
        return 1
```
这里把−θ命名为偏置b，但是请注意，偏置和权重w1、w2的作用是不 一样的。具体地说，w1和w2是控制输入信号的重要性的参数，而偏置是调 整神经元被激活的容易程度（输出信号为1的程度）的参数。比如，若b为 −0.1，则只要输入信号的加权总和超过0.1，神经元就会被激活。但是如果b 为−20.0，则输入信号的加权总和必须超过20.0，神经元才会被激活。像这样， 偏置的值决定了神经元被激活的容易程度。另外，这里我们将w1和w2称为权重， 将b称为偏置，但是根据上下文，有时也会将b、w1、w2这些参数统称为权重。
**非门**
``` python
def NAND(x1, x2):  
    x = np.array([x1, x2])  
    w = np.array([-0.5, -0.5])  
    b = 0.7  
    tmp = np.sum(w*x) + b  
    if tmp <= 0:  
        return 0  
    else:  
        return 1
```
**或门**
``` python
def OR(x1, x2):  
    x = np.array([x1, x2])  
    w = np.array([0.5, 0.5])  
    b = -0.2  
    tmp = np.sum(w*x) + b  
    if tmp <= 0:  
        return 0  
    else:  
        return 1
```
### 2.3 线性和非线性
图2-7中的○和△无法用一条直线分开，但是如果将“直线”这个限制条件去掉，就可以实现了。比如，我们可以像图2-8那样，作出分开○和△的空间。 感知机的局限性就在于它只能表示由一条直线分割的空间。图2-8这样弯 曲的曲线无法用感知机表示。另外，由图2-8这样的曲线分割而成的空间称为 非线性空间，由直线分割而成的空间称为**线性空间**。线性、非线性这两个术语在机器学习领域很常见，可以将其想象成图2-6和图2-8所示的直线和曲线。
![[I1OPN4D]3H}G}ZU3[B2D$JW.png]]
![[Z6VK0}OH%_H1)K@_@RD@2ZV.png]]

## 第三章 神经网络
### 3.1 从感知机到神经网络
用图来表示神经网络的话，如图3-1所示。我们把最左边的一列称为**输入层**，最右边的一列称为**输出层**，中间的一列称为中间层。中间层有时也称为**隐藏层**。“隐藏”一词的意思是，隐藏层的神经元（和输入层、输出 层不同）肉眼看不见。另外，本书中把输入层到输出层依次称为第0层、第 1层、第2层（层号之所以从0开始，是为了方便后面基于Python进行实现）。 图3-1中，第0层对应输入层，第1层对应中间层，第2层对应输出层。
![[MI[D@_RGD[TP()9}Q9(6VYT.png]]
### 3.2 激活函数
**激活函数的作用在于决定如何来激活输入信号的总和。** 所谓激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。然后这里也给出维基百科的定义：在计算网络中， 一个节点的激活函数(Activation Function)定义了该节点在给定的输入或输入的集合下的输出。标准的计算机芯片电路可以看作是根据输入得到开（1）或关（0）输出的数字电路激活函数。这与神经网络中的线性感知机的行为类似。然而，只有非线性激活函数才允许这种网络仅使用少量节点来计算非平凡问题。 在人工神经网络中，这个功能也被称为传递函数。 。
![[Pasted image 20220920151028.png]]
![[Pasted image 20220920151115.png]]
使用非线性激活函数是为了**增加神经网络模型的非线性因素**，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。
#### 3.2.1 sigmoid函数
sigmoid函数又称 Logistic函数，用于隐层神经元输出，取值范围为(0,1)，可以用来做二分类。
sigmoid函数表达式：$$\sigma (x) = \frac{1}{{1 + {e^{ - x}}}}$$
![[C0H4)GZTMVQH4Q_Y4CKD3FQ.png]]
**优点：**
1.  Sigmoid函数的输出在(0,1)之间，输出范围有限，优化稳定，可以用作输出层。
2.  连续函数，便于求导。

**缺点：**
1.  sigmoid函数在变量取绝对值非常大的正值或负值时会**出现饱和现象**，意味着函数会变得很平，并且对输入的微小改变会变得不敏感。在反向传播时，当梯度接近于0，权重基本不会更新，很容易就会**出现梯度消失**的情况，从而无法完成深层网络的训练。  
2.  **sigmoid函数的输出不是0均值的**，会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响。  
3.  **计算复杂度高**，因为sigmoid函数是指数形式。
#### 3.2.2 Tanh函数
Tanh函数也称为双曲正切函数，取值范围为[-1,1]。
Tanh函数定义如下：$$
\tanh (x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
$$
![[[WRR))LW3QTU%EZQBY_5A8K.png]]
Tanh函数是 0 均值的，因此实际应用中 Tanh 会比 sigmoid 更好。但是仍然存在梯度饱和与exp计算的问题。
#### 3.2.3 ReLU函数
ReLU函数定义如下：$$
f(x)=\max (0, x)
$$
![[]({8_NFQ_CWH7)6K0F}66X8.png]]
**优点：**
1.  使用ReLU的SGD算法的收敛速度比 sigmoid 和 tanh 快。
2.  在x>0区域上，不会出现梯度饱和、梯度消失的问题。
3.  计算复杂度低，不需要进行指数运算，只要一个阈值就可以得到激活值。

**缺点：**
1.  ReLU的输出**不是0均值**的。
2.  **Dead ReLU Problem(神经元坏死现象)**：ReLU在负数区域被kill的现象叫做dead relu。ReLU在训练的时很“脆弱”。在x<0时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新。
产生这种现象的两个原因：参数初始化问题；learning rate太高导致在训练过程中参数更新太大。
**解决方法**：采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。

参考  [知乎](https://zhuanlan.zhihu.com/p/337902763)
### 3.3 神经网络信号传递
![[U8Z$S9YVP@~H$_A7(3%)AZU.png]]
信号传递计算方法：$$a_1^{(1)}=w_{11}^{(1)} x_1+w_{12}^{(1)} x_2+b_1^{(1)}
$$
矩阵实现方法：$$
\boldsymbol{A}^{(1)}=\boldsymbol{X} \boldsymbol{W}^{(1)}+\boldsymbol{B}^{(1)}
$$
其中, $\boldsymbol{A}^{(1)} 、 \boldsymbol{X} 、 \boldsymbol{B}^{(1)} 、 \boldsymbol{W}^{(1)}$ 如下所示。
$$
\begin{aligned}
\boldsymbol{A}^{(1)} &=\left(\begin{array}{lll}
a_1^{(1)} & a_2^{(1)} & a_3^{(1)}
\end{array}\right), \boldsymbol{X}=\left(\begin{array}{ll}
x_1 & x_2
\end{array}\right), \boldsymbol{B}^{(1)}=\left(\begin{array}{lll}
b_1^{(1)} & b_2^{(1)} & b_3^{(1)}
\end{array}\right) \\
\boldsymbol{W}^{(1)} &=\left(\begin{array}{lll}
w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\
w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}
\end{array}\right)
\end{aligned}
$$

``` python 
import sys, os  
sys.path.append(os.pardir)    
import numpy as np  
import pickle  
from dataset.mnist import load_mnist  
from common.functions import sigmoid, softmax  
  
  
def get_data():  
    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)  
    return x_test, t_test  
  
  
def init_network():  
    with open("sample_weight.pkl", 'rb') as f:  
        network = pickle.load(f)  
    return network  
  
  
def predict(network, x):  
    W1, W2, W3 = network['W1'], network['W2'], network['W3']  
    b1, b2, b3 = network['b1'], network['b2'], network['b3']  
  
    a1 = np.dot(x, W1) + b1  
    z1 = sigmoid(a1)  
    a2 = np.dot(z1, W2) + b2  
    z2 = sigmoid(a2)  
    a3 = np.dot(z2, W3) + b3  
    y = softmax(a3)  
    return y  
  
  
x, t = get_data()  
print(x.shape,t.shape)  
network = init_network()  
accuracy_cnt = 0  
for i in range(len(x)):  
    y = predict(network, x[i])  
  
    p= np.argmax(y)  
    if p == t[i]:  
        accuracy_cnt += 1  
  
print("Accuracy:" + str(float(accuracy_cnt) / len(x)))
```
### 3.4 输出层激活函数
输出层所用的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元分类问题可以使用 sigmoid函数，多元分类问题可以使用 softmax函数。
**softmax函数**
$$
y_k=\frac{\exp \left(a_k\right)}{\sum_{i=1}^n \exp \left(a_i\right)}
$$
上面的softmax函数在计算机的运算上有一定的缺陷。这个缺陷就是溢出问题。softmax函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。比如，$e^{100}$的值 会超过20000，$e^{100}$ 会变成一个后面有40多个0的超大值，$e^{1000}$的结果会返回 一个表示无穷大的inf。如果在这些超大值之间进行除法运算，结果会出现“不 确定”的情况.在进行softmax的指数函数的运算时，加上（或者减去）某个常数并不会改变运算的结果。这里的$C^{}$可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值。softmax函数的输出是0.0到1.0之间的实数。并且，softmax 函数的输出值的总和是1。

一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。 并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此， 神经网络在进行分类时，输出层的softmax函数可以省略。在实际的问题中， 由于指数函数的运算需要一定的计算机运算量，因此输出层的softmax函数 一般会被省略。
### 3.5 神经网络层数、数量的确定
输入层和输出层的层数、大小是最容易确定的。每个网络都有一个输入层，一个输出层。输入层的神经元数目等于将要处理的数据的变量数。输出层的神经元数目等于每个输入对应的输出数。不过，确定隐藏层的层数和大小却是一项挑战。
下面是在分类问题中确定隐藏层的层数，以及每个隐藏层的神经元数目的一些原则：

-   在数据上画出分隔分类的期望边界。
-   将期望边界表示为一组线段。
-   线段数等于第一个隐藏层的隐藏层神经元数。
-   将其中部分线段连接起来（每次选择哪些线段连接取决于设计者），并增加一个新隐藏层。也就是说，每连接一些线段，就新增一个隐藏层。
-   每次连接的连接数等于新增隐藏层的神经元数目。
>_在神经网络中，当且仅当数据必须以非线性的方式分割时，才需要隐藏层。

隐藏层的层数与神经网络的效果/用途，可以用如下表格概括：![[Pasted image 20220920155108.png]]
-   **没有隐藏层**：仅能够表示线性可分函数或决策
-   **隐藏层数=1**：可以拟合任何“包含从一个有限空间到另一个有限空间的连续映射”的函数
-   **隐藏层数=2**：搭配适当的激活函数可以表示任意精度的任意决策边界，并且可以拟合任何精度的任何平滑映射
-   **隐藏层数>2**：多出来的隐藏层可以学习复杂的描述（某种自动特征工程）

层数越深，理论上拟合函数的能力增强，效果按理说会更好，但是实际上更深的层数可能会带来过拟合的问题，同时也会增加训练难度，使模型难以收敛。因此我的经验是，在使用BP神经网络时，最好可以参照已有的表现优异的模型，如果实在没有，则根据上面的表格，从一两层开始尝试，尽量不要使用太多的层数。在CV、NLP等特殊领域，可以使用CNN、RNN、attention等特殊模型，不能不考虑实际而直接无脑堆砌多层神经网络。**尝试迁移和微调已有的预训练模型，能取得事半功倍的效果**。

在隐藏层中使用太少的神经元将导致**欠拟合(underfitting)**。相反，使用过多的神经元同样会导致一些问题。首先，隐藏层中的神经元过多可能会导致**过拟合(overfitting)**。当神经网络具有过多的节点（过多的信息处理能力）时，训练集中包含的有限信息量不足以训练隐藏层中的所有神经元，因此就会导致过拟合。即使训练数据包含的信息量足够，隐藏层中过多的神经元会增加训练时间，从而难以达到预期的效果。显然，选择一个合适的隐藏层神经元数量是至关重要的。
**经验公式**
$$
N_h=\frac{N_s}{\left(\alpha *\left(N_i+N_o\right)\right)}
$$
其中： Ni 是输入层神经元个数；  
No是输出层神经元个数；  
Ns是训练集的样本数；  
α 是可以自取的任意值变量，通常范围可取 2-10。

总而言之，隐藏层神经元是最佳数量需要**自己通过不断试验获得**，建议从一个较小数值比如1到5层和1到100个神经元开始，如果欠拟合然后慢慢添加更多的层和神经元，如果过拟合就减小层数和神经元。此外，在实际过程中还可以考虑引入**Batch Normalization, Dropout, 正则化**等降低过拟合的方法。

通常，**对所有隐藏层使用相同数量的神经元就足够了**。对于某些数据集，拥有较大的第一层并在其后跟随较小的层将导致更好的性能，因为第一层可以学习很多低阶的特征，这些较低层的特征可以馈入后续层中，提取出较高阶特征。

[Hornik et al., 1989] 证明，只需二个包含足够多神经元的隐层，多层前馈网 络就能以任意精度逼近任意复杂度的连续函数.然而，如何设置隐层神经元的 个数仍是个未决问题，实际应用中通常靠"试错法" (trial-by-error) 调整.

## 第四章 神经网络的学习
### 4.1 数据驱动
![[1663687138590.png]]
人们以自己的经验和直觉为线索，通过反复试验推进工作。而机器学习的方法则极力避免人为介入，尝试从收集到的数据中发现答案（模式）。神经网络或深度学习则比以往的机器学习方法更能避免人为介入。
机器学习的方法中，由机器从收集到的数据中找出规律性。
深 度 学 习 有 时 也 称 为 端到端机器学习（end-to-end machine learning）。这里所说的端到端是指从一端到另一端的意思，也就是从原始数据（输入）中获得目标结果（输出）的意思。

机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验等。首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。为什么需要将数据分为**训练数据**和**测试数据**呢？因为我们追求的是模型的泛化能力。为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。另外，训练数据也可以称为**监督数据**。

**泛化能力**是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。

只对某个数据集过度拟合的状态称为**过拟合（over fitting）**。
### 4.2 损失函数
