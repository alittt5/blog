# 机器学习常用指标

-   **TP：True Positive**，真实标签为正例，模型也预测为正例，即真阳性——**还有得救**；
-   **FN：False Negative**，真实标签为正例，模型却预测为负例，即假阴性——**不忍直视**；
-   **FP：False Positive**，真实标签为负例，模型却预测为正例，即假阳性——**浪费资源**；
-   **TN：True Negative**，真实标签为负例，模型也预测为负例，即真阴性——**皆大欢喜**；
- **TPR（True Positive Rate）**，即真阳性率，常称为**召回率**（Recall）、查全率、命中率、或灵敏度（Sensitivity）。召回率针对的是原始的正样本，可以理解为有多少正样本被正确地预测。因此，原始的正样本要么被预测正确（TP），要么被预测错误（FN），则：
 $$
T P R=\frac{T P}{T P+F N}
$$
召回率高则说明漏检率低，它的一个应用场景便是疾病的检测，因为在在这种情况下我们是希望患病病例（正样本）被尽可能的检测出来，不然漏检的话后果很严重。
- **PPV（Positive Predictive Value）**，即阳性预测值，常称为**精确度**（Precision）或查准率。精确率针对的是预测的正样本，可以理解为被预测为阳性的样本中有多少是正确的。因此，被预测为阳性的样本要么原本就是正样本（TP），要么原本是负样本（FP），则：
$$
P P V=\frac{T P}{T P+F P}
$$
精确率高则说明误检率低，它的一个应用场景便是检索系统，譬如当我们使用浏览器搜索关键字时，检索出来的条目有多少时准确的，而上面上述的召回率则是所有准确的条目中有多少被成功的检索出来了。总的来说，召回率是尽可能的把正样本都查找出来；而精确率尽可能地保证预测的正确率。通常，这是一对相互矛盾的指标，比如当模型想要提升召回率这个指标时，就会倾向于将结果预测为正样本。然而，当你不是特别有把握时，如果强行预测为正样本，必将会导致不少的负样本被误判为正样本，从而导致精确率降低。反之亦然，当模型想要保持较高的精确率时，会比较顾虑，不会轻易的将当前样本判断为正样本。因此，必将会存在不少的正样本会漏检，从而导致召回率下降。
- **F1-Score**，即F1分数，也称为Dice系数或Dice相似度（Dice similarity coefficient）。F1分数可以定义为精确率和召回率的调和平均值，其值域为[0, 1]，通常值越大越好：
$$
F 1=\frac{2 \times P P V \times T P R}{P P V+T P R}
$$
以上公式称为F分数。可以明显的观察到，当 β=1 时，即等价于F1-Score，此时将精确率和召回率都视为同等重要。当然，在某些情况下，当我们认为精确率更重要时，我们可以适当得调小 β 的值，令其小于1；而当我们想凸显召回率时，则令其大于1。那么，什么情况下我们应该更注重哪个指标呢？下面简单的举几个例子帮大家理解下。比如在涉及到严重事故如癌病检测的系统中，我们要注重召回率，越高越好，因为我们不希望有病的人被漏诊；而对于垃圾邮箱检测系统来说，我们要注重精确度，越高越好，因为我们宁愿多看一封垃圾邮件，也不能错失一封重要的邮件；再举个不太恰当的例子，比如对于刑事诉讼审判系统来说，我国司法机关偏向于高召回率，即天网恢恢疏而不漏绝不放过一个坏人；而美国司法偏向于高精确率，即人权大于一切绝不冤枉一个好人；有时候很难说孰是孰非，更合理的做法便是提高F1分数。
- **TNR（True Negative Rate）**，即真阴性率，也称为特异度（Specificity）。特异度针对的是原始的负样本，可以理解为有多少负样本被正确地预测。同样地，原始的负样本中要么被正确地检测错误（TN），要么被错误的检测为正样本（FP），则：
$$
T N R=\frac{T N}{T N+F P}
$$
- **ACC（Accuracy）**，即准确率，这个应该是最常见且最容易理解的一个机器学习评价指标。准确率针对的是所有的样本被正确判断的概率？无非就两种情况是正确的，比如原来是正样本被模型判断为正样本（TP），或者原来是负样本被模型判断为负样本（TN），即：
$$
A C C=\frac{T P+T N}{T P+F N+F P+T N}
$$

一般我们很少会用准确率指标作为我们唯一的评判指标，因为单独地使用它有一个致命的缺陷。我举一个二类分割的例子，假设你在做一个缺陷检测分割的任务，一般来说缺陷的部位仅占原始图像区域的一小部分，如1%。那么当模型刚开始训练时，你会发现它的准确率可以达到80%多甚至时90%多都有可能，这是因为模型压根学不到东西，它只是习惯性地把所有的像素点都预测为全黑，这也是很多刚入门机器学习的新手特别喜欢问的一个问题，为什么我的模型准确率明明很高，可是预测出来的图片竟然是全黑（一本正经脸）？又比如在点击率预估评判中，通常来说用户点击的意愿时非常低，一般只有千分之几，如果单纯靠准确率来衡量广告投放的用户群体是否准确，这完全是不可取的。再举个例子帮大家把准确率和精确率联系起来，我以前在往穿越火线的时候，每次进入到开启队友可以互相伤害的房间时，我的爆头**准确率**总是接近百分百的，可惜我爆的是队友的头，此时只能说我的爆头**精确率**是零，毕竟我的目标是射击敌人。

-   **BA（Balanced Accuracy）**，即平衡准确率，它是针对准确率指标在不平衡数据集的一种改进。BA指标是综合考虑TPR（真阳率）和TNR（真阴率）这两个指标，即检测到患病占实际所有病患人数的比例以及检测到没病占实际所有健康患者人数的比例，其公式如下：

$$BA=\frac{TPR+TNR}2$$
- **MCC（Matthews correlation coefficient）**，即马修斯相关系数，它综合地考量了混淆矩阵中的四个基础评价指标。马修斯相关系数描述的是实际样本与预测样本之间的相关系数，值域为[-1, 1]。当值为1时代表模型完美预测，值为0时代表预测的结果较差甚至还不如随机预测来的好，值为-1时则代表预测结果极其糟糕基本完美避开正确答案，其公式如下：

$$
M C C=\frac{(T P \times T N)-(F P \times F N)}{\sqrt{(T P+F P) \times(T P+F N) \times(T N+F P) \times(T N+F N)}}
$$

通过比较MCC和F分数我们可以看出，MCC在某种意义上来说是全面的，它可以说是二分类问题的最佳度量指标。而F分数则忽略了TN，所以在某种情况下，比如TP=90, FP=4, TN=1, FN=5时，可以算法此时的F1值为0.9524，而MCC却只有0.14，这意味着当前分类器基本时弱相关的，它的预测接近随机预测的水平。因此，如果仅从F1值的得分来看，很容易误导我们说当前的分类器非常的棒~毕竟都上到95.24分。再考虑这种情况，当TP=1, FP=5, TN=90, FN=4时，此时F1值和MCC值分别为0.18和0.10，可见MCC在针对类别不平衡情况下的鲁棒性是更好的。

-   **P-R Curve**，即精确率-召回率曲线，其中横轴是召回率（TPR），纵轴是精确率（PPV）。PR曲线实质上是通过设置不同的阈值，最终将一系列的点汇聚起来连成一条线。我们知道，通过设置不同的阈值我们所划分的正样本或负样本均有所不同，一般来说将大于阈值的样本划分为正样本，而小于当前阈值的样本划分为负样本。需要注意的是，PR曲线对正负样本的比例异常敏感，即当正负样本的分布发生变化时，PR曲线的形状会发生巨大的变化。
-   **ROC（Receiver Operating Characteristic） Curve**，即受试者工作特性曲线，其来源于医学领域，起源于军事领域。ROC曲线的横轴为FPR，纵轴为TPR。与MCC一样，ROC实质上也同时考虑了混淆矩阵中的4个基准指标，因此它对于正负样本不均衡的情况下具有很强的鲁棒性，不容易受样本分布的波动而影响。其实ROC曲线跟P-R曲线一样，也是通过不断调整阈值来逐步产生不同的点，最后再将这些点连接起来便形成ROC曲线。

![](https://pic3.zhimg.com/80/v2-6b95b88f9d678381cdd204d1b9cc1232_720w.jpg)

摘自《百面机器学习》第029页表2.1 二 值分类模型的输出结果

假设有20个样本点，它的真实标签如上所述，即[p, p, n, ..., p, n]。我们知道，机器学习模型对输入样本点做出预测通常返回的一个概率值，它反映的是模型有多大把握将当前样本判定为正样本，下面梳理下ROC曲线的一种计算步骤：

Step 1：将模型的输出值按照概率值从高到低排序，如上图所述；

Step 2：首先将阈值定义为无穷大，此时所有的概率值均小于无穷大，这意味着模型将所有样本都预测为负样本，则TP和FP为0，根据公式可得TPR和FPR也为0，因此ROC曲线的第一个点必然是坐标原点（0, 0）；

Step 3：将阈值从大到小进行缩减，比如将采样间隔定位0.1，那么可以得到下一个阈值大小为0.9。统计上述表格，我们可以得到在原始的样本中，正例和负例的数量均为10。而对于预测样本来说，以0.9的阈值划分，可以得到模型的预测结果为[p, n, n, ..., n]，即模型仅将1号样本预测为正样本，则此时的TP=1，继而得到TPR=TP/(TP+FN)=1/10=0.1；同样地，由于没有正样本被预测错（因为模型将剩余19个样本均预测为负样本），所以FP=0，从而得到FPR=0。如此一来，我们便可以得到该阈值下对应的样本点为（0, 0.1）。以此类推，我们可以得到一系列的样本点，最后再将这些点连接起来便可以得到ROC曲线。

为了量化ROC曲线的表现，人们在ROC曲线的基础上引入一个标量AUC（Area Under Curve），顾名思义为曲线下的面积，它能够量化地反映出基于ROC曲线衡量出的模型性能。AUC值可通过对ROC横轴做（近似）积分求得，其值域一般为[0.5, 1]。当AUC=0.5时则说明此时模型的性能相当于在做随机预测，而当AUC取值接近1时说明当前分类器近乎完美。总的来说，ROC曲线对正负样本的分布不敏感，对于二分类问题来说它是非常适合作为类别不平衡数据集的评估指标。